{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "lip_cascade = cv2.CascadeClassifier('haarcascade_smile.xml')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to view/save picture\n",
    "\n",
    "video_capture = cv2.VideoCapture(1, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, img = video_capture.read()\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "\n",
    "#     img = cv2.imread('images/test2/surprise/0_32.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "    dets = detector(img)\n",
    "    for k, d in enumerate(dets):\n",
    "        shape = predictor(img, d)\n",
    "        xmouthpoints = [shape.part(x).x for x in range(0,68)]\n",
    "        ymouthpoints = [shape.part(x).y for x in range(0,68)]\n",
    "        maxx = max(xmouthpoints)\n",
    "        minx = min(xmouthpoints)\n",
    "        maxy = max(ymouthpoints)\n",
    "        miny = min(ymouthpoints) \n",
    "\n",
    "        crop_image = img[miny:maxy,minx:maxx]\n",
    "        crop_image = cv2.resize(crop_image, (224,224))\n",
    "    \n",
    "        cv2.imshow('face', crop_image)\n",
    "        cv2.imshow('original', img)\n",
    "    if cv2.waitKey(1) &  0xFF == ord('a'): \n",
    "        print('yeeted')\n",
    "        faces.append(crop_image)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "    \n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in faces:\n",
    "    img = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
    "    im = Image.fromarray(img)\n",
    "#     im.save(f\"H_{i}.jpg\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saveModel') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lableToName(label: str):\n",
    "    if label==0:\n",
    "        return 'joy'\n",
    "    elif label==1:\n",
    "        return 'neutral'\n",
    "    elif label==2:\n",
    "        return 'surprise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFace(img):\n",
    "    faces = detector(img)\n",
    "    for face in faces:\n",
    "        x1 = face.left()\n",
    "        y1 = face.top()\n",
    "        x2 = face.right()\n",
    "        y2 = face.bottom()\n",
    "        #cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "\n",
    "        landmarks = predictor(img, face)\n",
    "    pts = []\n",
    "    for n in range(0, 16):\n",
    "        x = landmarks.part(n).x\n",
    "        y = landmarks.part(n).y\n",
    "        pts.append((x, y))\n",
    "    for n in range(26, 21, -1):\n",
    "        x = landmarks.part(n).x\n",
    "        y = landmarks.part(n).y\n",
    "        pts.append((x, y))\n",
    "    for n in range(21, 16, -1):\n",
    "        x = landmarks.part(n).x\n",
    "        y = landmarks.part(n).y\n",
    "        pts.append((x, y))\n",
    "\n",
    "    x = landmarks.part(0).x\n",
    "    y = landmarks.part(0).y\n",
    "    pts.append((x, y))\n",
    "\n",
    "    pts = np.array(pts)\n",
    "\n",
    "    rect = cv2.boundingRect(pts)\n",
    "    x,y,w,h = rect\n",
    "    croped = img[y:y+h, x:x+w].copy()\n",
    "#     pts = pts - pts.min(axis=0)\n",
    "\n",
    "#     mask = np.zeros(croped.shape[:2], np.uint8)\n",
    "#     cv2.drawContours(mask, [pts], -1, (255, 255, 255), -1, cv2.LINE_AA)\n",
    "#     dst = cv2.bitwise_and(croped, croped, mask=mask)\n",
    "\n",
    "#     ## (4) add the white background\n",
    "#     bg = np.ones_like(croped, np.uint8)*255\n",
    "#     cv2.bitwise_not(bg,bg, mask=mask)\n",
    "#     dst2 = bg+ dst\n",
    "\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLips(img):\n",
    "    faces = detector(img)\n",
    "    for face in faces:\n",
    "        x1 = face.left()\n",
    "        y1 = face.top()\n",
    "        x2 = face.right()\n",
    "        y2 = face.bottom()\n",
    "        #cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "\n",
    "        landmarks = predictor(img, face)\n",
    "    pts = []\n",
    "    for n in range(48, 60):\n",
    "        x = landmarks.part(n).x\n",
    "        y = landmarks.part(n).y\n",
    "        pts.append((x, y))\n",
    "    \n",
    "    x = landmarks.part(48).x\n",
    "    y = landmarks.part(48).y\n",
    "    pts.append((x, y))\n",
    "\n",
    "    pts = np.array(pts)\n",
    "\n",
    "    rect = cv2.boundingRect(pts)\n",
    "    x,y,w,h = rect\n",
    "    croped = img[y:y+h, x:x+w].copy()\n",
    "    \n",
    "#     pts = pts - pts.min(axis=0)\n",
    "\n",
    "#     mask = np.zeros(croped.shape[:2], np.uint8)\n",
    "#     cv2.drawContours(mask, [pts], -1, (255, 255, 255), -1, cv2.LINE_AA)\n",
    "#     dst = cv2.bitwise_and(croped, croped, mask=mask)\n",
    "\n",
    "#     ## (4) add the white background\n",
    "#     bg = np.ones_like(croped, np.uint8)*255\n",
    "#     cv2.bitwise_not(bg,bg, mask=mask)\n",
    "#     dst2 = bg+ dst\n",
    "    return croped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lableToName(label: str):\n",
    "    if label==0:\n",
    "        return 'joy'\n",
    "    elif label==1:\n",
    "        return 'neutral'\n",
    "    elif label==2:\n",
    "        return 'surprise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "lips_points = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n0 -> neutral\\n1 -> joy\\n2 -> surprise\\n\\n'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "0 -> neutral\n",
    "1 -> joy\n",
    "2 -> surprise\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saveModel') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no face detected in  12_41.jpg\n",
      "no face detected in  1_34.jpg\n",
      "no face detected in  8_67.jpg\n",
      "no face detected in  H_10.jpg\n",
      "no face detected in  H_11.jpg\n",
      "no face detected in  H_21.jpg\n",
      "no face detected in  H_22.jpg\n",
      "no face detected in  H_23.jpg\n",
      "no face detected in  H_24.jpg\n",
      "no face detected in  H_9.jpg\n"
     ]
    }
   ],
   "source": [
    "img_path = 'images/final/surprise/'\n",
    "save_path = 'images/lips/surprise/'\n",
    "for i in os.listdir(img_path):\n",
    "    img = cv2.imread(img_path+i)\n",
    "    try:\n",
    "        lips_img = getLips(img)    \n",
    "        lips_img = cv2.resize(lips_img, (64,64))\n",
    "        lips_img = cv2.cvtColor(lips_img, cv2.COLOR_BGR2GRAY) \n",
    "        im = Image.fromarray(lips_img)\n",
    "        im.save(save_path+i)\n",
    "#         lips_points.append(lips_img)\n",
    "#         labels.append(2)\n",
    "    except:\n",
    "        print('no face detected in ', i)\n",
    "    \n",
    "#     face_img = getFace(img)\n",
    "#     print(i)\n",
    "#     plt.imshow(lips_img)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "surprise\n",
      "surprise\n",
      "joy\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "joy\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "surprise\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "joy\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n",
      "surprise\n"
     ]
    }
   ],
   "source": [
    "video_capture = cv2.VideoCapture(1, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, img_frame = video_capture.read()\n",
    "    img_frame = cv2.cvtColor(img_frame, cv2.COLOR_BGR2GRAY) \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        dets = detector(img_frame)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for k, d in enumerate(dets):\n",
    "        shape = predictor(img_frame, d)\n",
    "        xmouthpoints = [shape.part(x).x for x in range(0,68)]\n",
    "        ymouthpoints = [shape.part(x).y for x in range(0,68)]\n",
    "        maxx = max(xmouthpoints)\n",
    "        minx = min(xmouthpoints)\n",
    "        maxy = max(ymouthpoints)\n",
    "        miny = min(ymouthpoints) \n",
    "\n",
    "        crop_image_ = img_frame[miny:maxy,minx:maxx]\n",
    "        crop_image = cv2.resize(crop_image_, (224,224))\n",
    "        \n",
    "        cv2.imshow('face', crop_image)\n",
    "        cv2.imshow('non-resized', crop_image_)\n",
    "\n",
    "        cv2.imshow('camera feed', img_frame)\n",
    "    \n",
    "        img_array = image.img_to_array(crop_image)\n",
    "        img_batch = np.expand_dims(img_array, axis=0)\n",
    "        emo = np.argmax(model.predict(img_batch))\n",
    "        print(lableToName(emo))\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n"
     ]
    }
   ],
   "source": [
    "video_capture = cv2.VideoCapture(1)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, img = video_capture.read()\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "    \n",
    "    lips_img = getLips(img)    \n",
    "    \n",
    "    lips_img = cv2.resize(lips_img, (64,64))\n",
    "\n",
    "    cv2.imshow('face', lips_img)\n",
    "\n",
    "    cv2.imshow('camera feed', img)\n",
    "\n",
    "    img_array = image.img_to_array(lips_img)\n",
    "    img_batch = np.expand_dims(img_array, axis=0)\n",
    "    emo = np.argmax(model.predict(img_batch))\n",
    "    \n",
    "    print(lableToName(emo))\n",
    "#     print(emo)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bitd9f5db4c6a59403998988a5c56626fe9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
